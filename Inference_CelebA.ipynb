{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d8de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54189786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint',\n",
       " 'shallow_model.h5py.data-00000-of-00001',\n",
       " 'shallow_model.h5py.index']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('model_lfw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d46a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 128\n",
    "img_width = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1318551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 16:43:32.967418: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 16:43:33.748360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11414 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:83:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'../../Datasets/CelebA/img_align_celeba/000001.jpg'\n",
      "b'../../Datasets/CelebA/img_align_celeba/000002.jpg'\n",
      "b'../../Datasets/CelebA/img_align_celeba/000003.jpg'\n",
      "b'../../Datasets/CelebA/img_align_celeba/000004.jpg'\n",
      "b'../../Datasets/CelebA/img_align_celeba/000005.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 16:43:34.687578: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../Datasets/CelebA/img_align_celeba/*.jpg'\n",
    "list_ds = tf.data.Dataset.list_files(data_dir, shuffle=False)\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd89b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data: 121559\n",
      "number of validating data: 40519\n",
      "number of testing data: 40519\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list_ds)\n",
    "train_size = int(image_count * 0.6)\n",
    "val_size = int(image_count * 0.2)\n",
    "test_size = int(image_count * 0.2)\n",
    "train_ds = list_ds.take(train_size)\n",
    "val_ds = list_ds.skip(train_size).take(val_size)\n",
    "test_ds = list_ds.skip(train_size+val_size).take(test_size)\n",
    "print(\"number of training data:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"number of validating data:\", tf.data.experimental.cardinality(val_ds).numpy())\n",
    "print(\"number of testing data:\", tf.data.experimental.cardinality(test_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2a47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "def process_path(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, img\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)\n",
    "test_ds = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ffc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_pad (ZeroPadding2D)    (None, 134, 134, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_conv (Conv2D)          (None, 64, 64, 64)        9472      \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)    (None, 66, 66, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)    (None, 32, 32, 64)        0         \n",
      "=================================================================\n",
      "Total params: 9,728\n",
      "Trainable params: 9,600\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 32, 32, 64)        9728      \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 128, 128, 3)       107456    \n",
      "=================================================================\n",
      "Total params: 117,184\n",
      "Trainable params: 107,328\n",
      "Non-trainable params: 9,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_generator_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides = 2, padding='same', use_bias=False, activation='selu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=2, padding='same', use_bias=False, activation='selu'))\n",
    "    return model\n",
    "\n",
    "encoder = tf.keras.applications.resnet50.ResNet50(\n",
    "            input_shape = (224,224,3), \n",
    "            weights = 'imagenet', \n",
    "            include_top = False, \n",
    "            pooling = 'avg'\n",
    "        )\n",
    "\n",
    "encoder = tf.keras.applications.resnet50.ResNet50(\n",
    "            input_shape = (img_height,img_width,3), \n",
    "            weights = 'imagenet', \n",
    "            include_top = False, \n",
    "            pooling = 'avg'\n",
    "        ).layers[:7]\n",
    "\n",
    "encoder = tf.keras.Sequential(encoder)\n",
    "encoder.summary()\n",
    "encoder.layers[-1].output_shape[1:]\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, layer):\n",
    "        super(Autoencoder, self).__init__() \n",
    "        self.encoder = tf.keras.applications.resnet50.ResNet50(\n",
    "            input_shape = (img_height,img_width,3), \n",
    "            weights = 'imagenet', \n",
    "            include_top = False, \n",
    "            pooling = 'avg'\n",
    "        ).layers[:layer]\n",
    "        self.encoder = tf.keras.Sequential(self.encoder)\n",
    "        self.encoder.trainable = False\n",
    "        self.decoder = make_generator_model(self.encoder.layers[-1].output_shape[1:])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(7)\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.MeanSquaredError())\n",
    "autoencoder.build((1,img_height,img_width,3))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b044e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 64)        102400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 128, 128, 3)       4800      \n",
      "=================================================================\n",
      "Total params: 107,456\n",
      "Trainable params: 107,328\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_generator_model((32,32,64))\n",
    "model.load_weights('model_lfw/shallow_model.h5py')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4afce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 16:43:38.726887: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-04-14 16:43:39.590381: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2023-04-14 16:43:40.801914: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "considered_images = []\n",
    "plain_feature = []\n",
    "for img, label in val_ds.take(1):\n",
    "    image = img.numpy()\n",
    "    for i in range(10):\n",
    "        temp_img = image[i]\n",
    "        considered_images.append(temp_img)\n",
    "        temp_img = np.expand_dims(temp_img, axis=0)\n",
    "        feat = encoder.predict(temp_img) \n",
    "        plain_feature.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9734c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(1, 32, 32, 64)\n",
      "<class 'numpy.ndarray'>\n",
      "(10, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(len(plain_feature))\n",
    "print(plain_feature[0].shape)\n",
    "# model.summary()\n",
    "plain_feature = np.vstack(plain_feature)\n",
    "print(type(plain_feature))\n",
    "print(plain_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81216f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def fairRR(arr, eps, num_int, num_bit, mode = 'dp'):\n",
    "    r = arr.shape[1]\n",
    "    num_pt = arr.shape[0]\n",
    "    \n",
    "    def float_to_binary(x, m=num_int, n=num_bit - num_int - 1):\n",
    "        x_abs = np.abs(x)\n",
    "        x_scaled = round(x_abs * 2 ** n)\n",
    "        res = '{:0{}b}'.format(x_scaled, m + n)\n",
    "        if x >= 0:\n",
    "            res = '0' + res\n",
    "        else:\n",
    "            res = '1' + res\n",
    "        return res\n",
    "\n",
    "    # binary to float\n",
    "    def binary_to_float(bstr, m=num_int, n=num_bit - num_int - 1):\n",
    "        sign = bstr[0]\n",
    "        bs = bstr[1:]\n",
    "        res = int(bs, 2) / 2 ** n\n",
    "        if int(sign) == 1:\n",
    "            res = -1 * res\n",
    "        return res\n",
    "\n",
    "    def string_to_int(a):\n",
    "        bit_str = \"\".join(x for x in a)\n",
    "        return np.array(list(bit_str)).astype(int)\n",
    "\n",
    "    def join_string(a, num_bit=num_bit, num_feat=r):\n",
    "        res = np.empty(num_feat, dtype=\"S10\")\n",
    "        # res = []\n",
    "        for i in range(num_feat):\n",
    "            # res.append(\"\".join(str(x) for x in a[i*l:(i+1)*l]))\n",
    "            res[i] = \"\".join(str(x) for x in a[i * num_bit:(i + 1) * num_bit])\n",
    "        return res\n",
    "    \n",
    "    def alpha_tr1(r, eps, l):\n",
    "        return np.exp( ( eps - r*eps*(l-1) ) /(2*r*l) )\n",
    "\n",
    "    def alpha(r, eps, l):\n",
    "        nu = 2*( np.sqrt( 6*np.log(10) /(2*r) ) )\n",
    "        sum_ = 0\n",
    "        for k in range(l):\n",
    "            sum_ += np.exp(2 * eps*k / l)\n",
    "        return np.sqrt(((1-nu)*eps + r*l) / (2*r * sum_))\n",
    "\n",
    "    alpha_ = alpha_tr1(r=r, eps=eps, l=num_bit) if mode == 'dp' else alpha(r=r, eps=eps, l=num_bit)\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_to_binary)\n",
    "    binary_to_float_vec = np.vectorize(binary_to_float)\n",
    "\n",
    "    feat_tmp = float_to_binary_vec(arr)\n",
    "    feat = np.apply_along_axis(string_to_int, 1, feat_tmp)\n",
    "    \n",
    "    index_matrix = np.array(range(num_bit))\n",
    "    index_matrix = np.tile(index_matrix, (num_pt, r))\n",
    "    p = 1 / (1 + alpha_ * np.exp(index_matrix * eps / num_bit))\n",
    "    p_temp = np.random.rand(p.shape[0], p.shape[1])\n",
    "    perturb = (p_temp > p).astype(int)\n",
    "    print(feat[0][:10])\n",
    "    print(perturb[0][:10])\n",
    "    perturb_feat = (perturb + feat) % 2\n",
    "    print(perturb_feat[0][:10])\n",
    "    perturb_feat = np.apply_along_axis(join_string, 1, perturb_feat)\n",
    "    perturb_feat = binary_to_float_vec(perturb_feat)\n",
    "    print(arr[0][:2])\n",
    "    print(perturb_feat[0][:2])\n",
    "    return perturb_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee12967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 32, 64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "877e711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_plain = np.reshape(plain_feature, newshape=(plain_feature.shape[0], plain_feature.shape[1]*plain_feature.shape[2]*plain_feature.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f372683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 0]\n",
      "[0 0 0 0 1 0 1 1 1 1]\n",
      "[0 0 0 0 1 1 0 0 0 1]\n",
      "[1.8467633 0.       ]\n",
      "[3.0625 1.9375]\n"
     ]
    }
   ],
   "source": [
    "perturb_feat = fairRR(arr=temp_plain, eps=10.0, num_int=5, num_bit=10, mode = 'dp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a15a2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_matrix = np.reshape(perturb_feat, newshape=plain_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7de3abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img = model.predict(np.expand_dims(perturb_matrix[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce1e0ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f91ed81d390>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(pred_img).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780174b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
